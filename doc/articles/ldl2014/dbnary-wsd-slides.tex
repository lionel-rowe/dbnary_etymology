\documentclass{beamer}

\usepackage{beamerthemesplit}

\usetheme{Boadilla} 

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[utf8]{vietnam}
\usepackage[english]{babel}
\usepackage{calc}
\usepackage{ifthen}
\usepackage{tikz}
\usepackage{eurosym}

\usepackage{alltt}

\usepackage{verbatim}

\usepackage{mdframed}

\usepackage[algoruled,vlined,noend]{algorithm2e}
%\SetKwIF{Si}{SinonSi}{Sinon}{si}{}{sinon si}{sinon}{}
%\SetKwFor{Pour}{pour}{}{}
%\SetKwFor{RepeterNFois}{répéter}{fois}{}
%\SetKwFor{TantQue}{tant que}{}{}%
%\SetKwFor{PourChaque}{pour chaque}{}{}%

\usepackage{CJKutf8}

\usepackage[overlap, CJK]{ruby}
\usepackage{CJKulem}

\renewcommand{\rubysep}{-0.2ex}

\newenvironment{SChinese}{%
  \CJKfamily{gbsn}%
  \CJKtilde
  \CJKnospace}{}
\newenvironment{TChinese}{%
  \CJKfamily{bsmi}%
  \CJKtilde
  \CJKnospace}{}
\newenvironment{Japanese}{%
  \CJKfamily{min}%
  \CJKtilde
  \CJKnospace}{}
\newenvironment{Korean}{%
  \CJKfamily{mj}}{}
  
  
\DeclareMathOperator*{\argmax}{arg\!\max}
  
\newcommand{\cntext}[1]{\begin{CJK}{UTF8}{}\begin{Japanese}#1\end{Japanese}\end{CJK}}

\pgfdeclareimage[height=0.8cm]{logo}{img/getalp}
\pgfdeclareimage[height=1.5cm]{logobig}{img/getalp} 

\logo{\pgfuseimage{logo}}

\title[Disambiguating Translations in DBnary]{Attaching Translations to Proper Lexical Senses in DBnary}
%\subtitle{Wiktionnaire, RDF, Linked Data: une colonne vertébrale pour le lexique ?}
\author[Tchechmedjiev et al. \hspace{-2mm}]{Andon Tchechmedjiev, Gilles Sérasset, Jérôme Goulian, Didier Schwab}
\institute[\fontsize{1.8mm}{1mm}\selectfont GETALP-LIG, Grenoble]{GETALP-LIG, Univ Grenoble Alpes, France}
\titlegraphic{\pgfuseimage{logobig}}
\date{May 27, 2014}

% Delete this, if you do not want the table of contents to pop up at
% the beginning of each subsection:
\AtBeginSection[]
{
  \begin{frame}<beamer>
    \frametitle{}
    \tableofcontents[currentsection]
  \end{frame}
}

\begin{document}

\frame{\titlepage}


\section{Introduction}



\section{Overview of the DBnary dataset}

\frame{\frametitle{What is Dbnary?}
\begin{itemize}
\item Extracting lexical data from 12 different Wiktionary language editions
	\begin{itemize} 
	\item Wiktionary
	\end{itemize}
\item ``Fine grained'' interoperability\\
	\begin{itemize} 
	\item RDF
	\end{itemize}
\item A standard model
	\begin{itemize} 
	\item LEMON (inspired by LMF)
	\end{itemize}
\item Multilingual
	\begin{itemize} 
	\item Extraction of several language edition of Wiktionary
	\end{itemize}
\item Available, Usable et Used
	\begin{itemize} 
	\item Linked Data
	\end{itemize}
	
\end{itemize}
}

\frame{  
\frametitle{Dbnary: Wiktionary as a lexical graph}

     \includegraphics<1>[width=\linewidth]{img/cat1.png}
     \includegraphics<2>[width=\linewidth]{img/cat2.png}
     \includegraphics<3>[width=\linewidth]{img/cat3.png}
     \includegraphics<4>[width=\linewidth]{img/cat4.png}
     \includegraphics<5>[width=\linewidth]{img/cat5.png}
     \includegraphics<6>[width=\linewidth]{img/cat6.png}
     \includegraphics<7>[width=\linewidth]{img/cat_struct}
     \includegraphics<8>[width=\linewidth]{img/cat_syn}
     \includegraphics<9>[width=\linewidth]{img/cat_translations}

}

\frame{  
\frametitle{LEMON}

     \includegraphics<1>[width=0.95\linewidth]{img/lemon-core.png}
     \includegraphics<2>[width=0.95\linewidth]{img/lemon-descr.png}
     \includegraphics<3>[width=0.95\linewidth]{img/dbnary-lemon-xtension.pdf}

}

\frame{
\frametitle{Extraction principles}
\begin{itemize}
\item 10 editions extracted: en, fr, de, ru, it, pt, fi, el
\item Extraction from wikimedia dumps
\item Every edition is updated with the latest dump every 10-15 days
\item Extraction is performed on-line, the files are available on-the-fly
\item A Linked Data server holds older version of the dataset
\end{itemize}
}

\frame{  
\frametitle{Dataset Size (as of today)}

\begin{table}[htb]
\begin{tabular}{lrrrr}
 & \textbf{Entries} & \textbf{Vocables} & \textbf{Senses} & \textbf{Translations}\\
 \hline
\textbf{eng} & 527067 & 504594 & 421232 & 1126463 \\
\textbf{fra} & 273822 & 283847 & 358921 & 464956 \\
\textbf{deu} & 135103 & 201736 & 95593 & 471892 \\
\textbf{rus} & 127271 & 139235 & 99243 & 325345 \\
\textbf{ell} & 74056 & 74800 & 34932 & 55652 \\
\textbf{fin} & 48164 & 48050 & 56559 & 118728 \\
\textbf{por} & 43042 & 44061 & 77631 & 225065 \\
\textbf{ita} & 25279 & 31935 & 35061 & 57796 \\
\end{tabular}
\caption{Number of resources by type and language, sorted by number of lexical entries.}\label{globalsize}
\end{table}
}

\frame{  
\frametitle{Dataset Size (as of today)}
\begin{table}[htb]
\begin{tabular}{lrrrrrr}
 & \textbf{syn}  & \textbf{ant} & \textbf{hyper} & \textbf{hypo} & \textbf{mero} & \textbf{holo} \\
 \hline
\textbf{eng} & 31461& 6877& 959& 1103& 114& 0 \\ 
\textbf{fra} & 30088& 6735& 8215& 3557& 943& 1847 \\ 
\textbf{deu} & 27516& 14315& 30202& 9509& 0& 0 \\ 
\textbf{rus} & 22631& 9204& 21028& 4756& 0& 0 \\ 
\textbf{ell} & 3975& 1116& 0& 0& 0& 0 \\ 
\textbf{fin} & 2255& 0& 0& 0& 0& 0 \\ 
\textbf{por} & 3527& 575& 6& 3& 0& 0 \\ 
\textbf{ita} & 7091& 2337& 0& 0& 0& 0 \\ 
\end{tabular}
\caption{Number of lexicon-semantic relations. Languages are sorted according to their number of lexical entries.}\label{nymsize}
\end{table}
}

\frame{  
\frametitle{Dataset Size (as of today)}
\begin{table}[htb]
\begin{tabular}{lrrrrrrrrrrrrr}\tiny
\textbf{Source/Target}  & \textbf{deu} & \textbf{ell} & \textbf{eng} & \textbf{fin} & \textbf{fra} & \textbf{ita} & \textbf{por} & \textbf{rus}& \textbf{others} & \textbf{Total} & \textbf{\# of target languages}\\
\textbf{eng} & 62501 & 23794 & 1 & 74938 & 57959 & 37467 & 30256 & 74837 & 764710 & 1126463 & 1143\\
\textbf{fra} & 34608 & 7063 & 74687 & 7589 & 12 & 18806 & 17784 & 7783 & 296624 & 464956 & 952\\
\textbf{deu} & 0 & 2675 & 81015 & 4947 & 67143 & 41485 & 8872 & 17354 & 248401 & 471892 & 355\\
\textbf{rus} & 23056 & 3295 & 48559 & 3966 & 14776 & 12643 & 5567 & 0 & 206709 & 318571 & 490\\
\textbf{ell} & 2242 & 2 & 10090 & 1056 & 8436 & 1470 & 1149 & 1315 & 29892 & 55652 & 246\\
\textbf{fin} & 8046 & 918 & 30103 & 0 & 6700 & 3856 & 2196 & 7997 & 58912 & 118728 & 329\\
\textbf{por} & 7000 & 2816 & 11284 & 4607 & 8720 & 7096 & 4 & 4396 & 179142 & 225065 & 695\\
\textbf{ita} & 4619 & 506 & 17539 & 925 & 4461 & 75 & 1219 & 938 & 27514 & 57796 & 315\\
\end{tabular}
\caption{Number of translations from/to the 8 currently extracted languages. Source languages are sorted according to their number of lexical entries. Target languages are sorted by their ISO 639-3 language code. The number of different target languages is also given.}\label{tradsize1}
\end{table}
}

\frame{  
\frametitle{Dataset Size (as of today)}
\begin{table}[htb]
\begin{tabular}{lrrrrrr}
\textbf{Source/Target}  & \textbf{por} & \textbf{rus}& \textbf{others} & \textbf{Total} & \textbf{\# of lang}\\
\textbf{eng} & 30256 & 74837 & 764710 & 1126463 & 1143\\
\textbf{fra} & 17784 & 7783 & 296624 & 464956 & 952\\
\textbf{deu} & 8872 & 17354 & 248401 & 471892 & 355\\
\textbf{rus} &  5567 & 0 & 206709 & 318571 & 490\\
\textbf{ell} &  1149 & 1315 & 29892 & 55652 & 246\\
\textbf{fin} &  2196 & 7997 & 58912 & 118728 & 329\\
\textbf{por} &  4 & 4396 & 179142 & 225065 & 695\\
\textbf{ita} &  1219 & 938 & 27514 & 57796 & 315\\
\end{tabular}
\caption{Number of translations from/to the 8 currently extracted languages. Source languages are sorted according to their number of lexical entries. Target languages are sorted by their ISO 639-3 language code. The number of different target languages is also given.}\label{tradsize2}
\end{table}
}

\frame{  
\frametitle{Quality of the data}

\begin{itemize}
\item Evaluating the quality of the data is  difficult
\item Extraction quality $\ne$ data quality
\item No particular applications at the moment
\item No alignments yet with other resources (Wordnet, Jeuxdemots, ...)
\item However, several direction are available
\end{itemize}
}

\frame{  
\frametitle{Data quality (yesterday)}
\begin{table}[htb]
\begin{tabular}{lrr}
\textbf{language} & \textbf{\# of transl.}\\
 \hline
\textbf{eng} & 5110 (99.1 \%) \\
\textbf{fra} & 5799 (107.0 \%) \\
\textbf{deu} & 10287 (99.2 \%)\\
\textbf{rus} & 8436 (24811.7 \%) \\
\textbf{ell} & 2598 (64.3 \%) \\
\textbf{fin} & 7245 (28980 \%) \\
\textbf{por} & 17720 (93.2 \%) \\
\textbf{ita} & 7855 (3167.3 \%) 
\end{tabular}
\caption{Extracted translations vs interwiki links, on a random sample of 1000 entries.}\label{iwlinks}
\end{table}
}

\section{Attaching Translation to their Proper Lexical Sense}

\subsection{Problem}

\frame{  
\frametitle{The problem}

\only<1>{\begin{mdframed}\includegraphics[width=\linewidth]{img/dbnarystrc1}\end{mdframed}}
\only<2>{\begin{mdframed}\includegraphics[width=\linewidth]{img/dbnarystrc2}\end{mdframed}}
\begin{overprint}
\onslide<3>
\begin{center}
\[T, \mbox{ the set of translation relations}\]
\[
\forall T_i \in T, S=Senses(Source(T_i)): 
\]
\[
  M_S = \max_{S_k\in S}(Score((Gloss(T_i),Def(S^k))),
\]
\vfill
\[
\argmax_{S_i\in S}^\delta \{Score(Gloss(T_i),Def(S^k))\}=
\]
\[
\{S^k\in S|  M_S > Score((Gloss(T_i),Def(S^k)) > M_S-\delta \}
\]
\end{center}
\end{overprint}	
}


\subsection{Selecting a Similarity Measure}
\frame{
\frametitle{Selecting a Similarity Measure}
\textbf{Cues available}
\begin{itemize}
\item Some translation sources contain glosses that reprise a part of the definition of the corresponding senses 
\item We can compare the gloss to sense definitions
\end{itemize}
\vfill
\textbf{Constraints}
\begin{itemize}
	\item Gloss overlap measure
	\item No lemmatisation/stemming $\Rightarrow$ approximate matching
\end{itemize}
}

\frame{
\frametitle{Selecting a Similarity Measure}

\textbf{Possibilities}
\begin{itemize}
	\item Text similarity $\rightarrow$ Strings, no word distinctions
	\item Simple overlap $\rightarrow$ Few matches on surface forms
	\item Hybrid measures $\Rightarrow$ Combine word overlap + Text similarity
\end{itemize}
\vfill
\textbf{Hybrid measure}
\begin{itemize}
\item An overlap measure (Lesk, Jaccard, Tverski, etc.)
\item Set cardinality $\Leftrightarrow$ Sum of pairwise word textual similarities
\item What is the most suitable textual similarity measure?
\end{itemize}
}

\frame{
\frametitle{Our Similarity Measure}

\textbf{Tversky index}
\begin{center}

\[
	\frac{|d_1\cap d_2|}{|d_1\cap d_2| + \alpha |d_1 - d_2| + \beta |d_2 - d_1|}
\]
\end{center}
\vfill
\textbf{Hybrid Tverski}
\[|d_1\cap d_2| \longrightarrow
		\sum_{(x,y)\in d_1 \times d_2}{sim(x,y)}\]

}


\subsection{Parameters}
\begin{frame}
\frametitle{Parameters to Estimate}
\begin{itemize}
  \item $\delta$ Similarity value range around the best disambiguation
  \vfill
  \item $\alpha$ \& $\beta$  Relative importance of the differences in one or the other set of words. If we want a Tverski index that remains between 0 and 1, then we must have $\alpha=1-\beta$.
  \vfill
  \item $sim$ The similarity measure to use, we have the choice between
  \begin{itemize}
	  \item Jaro-Winkler~(FTiJW)
	  \item Scaled-Levenshtein distance~(FTiLs)
	  \item Monge-Elkan~(FTiME)
	  \item Normalized Longest Common Substring~(FTiLcss)
	  \item None~(Ti)
  \end{itemize}
\end{itemize}
\end{frame}

\section{Evaluation}

\subsection{Extraction of an Endogeneous Gold Standard}
\begin{frame}
	\frametitle{Extraction of an Endogeneous Gold Standard}
	\begin{itemize}
	\item Translation links with glosses $\cap$ Translation links with sense numbers
	\begin{itemize}
		\item Gold standard -- entries built from sense numbers
		\item Algorithm -- sense numbers removed, only glosses considered
	\end{itemize}
	
	\vfill
	
		\item Three language editions with sufficient data:
		\begin{itemize}
			\item Finnish -- 115,~550
			\item Portuguese -- 69,~172
			\item French -- 28,~114
		\end{itemize}
	\vfill
	\item Gold standard and algorithm output in Trec\_eval format
	\end{itemize}
\end{frame}
\subsection{Experimental Protocol}
\begin{frame}
	\frametitle{Experimental Protocol}
	\begin{enumerate}
	\item Estimate independent parameters -- \(\alpha\), \(\beta\), \(sim\)
	\vfill
	\item Evaluate the results w.r.t. dependent parameter \(delta\)
	\vfill
	\item Compare results to textual similarity hybrid measures w.r.t \(delta\)
	\begin{itemize}
	\item Level 2 Jaro-Winkler~(L2JW)
	\item Level 2 Scaled Levenstein distance~(L2Ls)
	\item Level 2 Monge-Elkan~(L2ME)
	\end{itemize}
	\end{enumerate}
\end{frame}

\subsection{Parameter Estimation}

\begin{frame}
	\frametitle{Parameter Estimation -- \(sim\)}
	\begin{table}
	{\centering \footnotesize
	\begin{tabular}{|c|c|c|c|}
	\hline &French&Portuguese&Finnish\\
	\hline &F1&F1&F1\\
	\hline FTiJW&0.7853&0.8079&0.9479\\
	\hline FTiLcss&0.7778&0.7697&0.9495\\
	\hline FTiLs&\textbf{0.7861}&\textbf{0.8176}&\textbf{0.9536}\\
	\hline FTiME&0.7684&0.7683&0.9495\\
	\hline Ti&0.7088&0.7171&0.8806\\
	\hline 
	\end{tabular} 
	\caption{\scriptsize F\_1 score for French, Finnish and Portuguese for each textual similarity measure}
	\label{tab:expe1}
	}
	\end{table}
\end{frame}


\begin{frame}
	\frametitle{Parameter Estimation -- \(\alpha\) \& \(\beta\)}
	\begin{figure}\centering
	 \begin{tikzpicture}
		            \node<1-2>[anchor=south west,inner sep=0] at (0,0) {	\includegraphics[width=0.7\textwidth]{alphabetafig}};
		            \draw<2>[green,very thick,rounded corners] (1.4,2.3) rectangle (1.9,5.4);
		        \end{tikzpicture}

	\caption{\scriptsize F1 score for Finnish, French and Portuguese w.r.t. \(\alpha\) and \(\beta\).}
	\label{fig.1}
	\end{figure}
\end{frame}



\subsection{Results}

\begin{frame}
	\frametitle{Results w.r.t \(\delta\)}
	\begin{figure}
	\centering
	%TikZ picture pour entourer le meilleur résultat en rouge en overlay
	        \begin{tikzpicture}
	            \node<1-2>[anchor=south west,inner sep=0] at (0,0) {\includegraphics[width=.40\textwidth]{french}\includegraphics[width=.39	\textwidth]{portuguese}};
	            \draw<2>[orange,thick,rounded corners] (.9,2.65) rectangle (1.2,2.95);
	            \draw<2>[orange,thick,rounded corners] (5.6,2.68) rectangle (5.9,2.98);
	        \end{tikzpicture}
	        
	        \begin{tikzpicture}
	            \node<1-2>[anchor=south west,inner sep=0] at (0,0) {	\includegraphics[width=.40\textwidth]{finnish}};
	            \draw<2>[orange,thick,rounded corners] (.8,2.50) rectangle (1.1,2.80);
	        \end{tikzpicture}	        
	\caption{\scriptsize F1 score against delta for our measure and other Level 2 Measures.}
	\label{fig.2}
	\end{figure}
\end{frame}

\section{Conclusion}

\frame{  
\frametitle{Conclusion}

\begin{itemize}
\item 
\end{itemize}
}


\end{document}




