\documentclass[10pt, a4paper]{article}
\usepackage{lrec2006}
\usepackage{graphicx}

\usepackage[utf8]{inputenc}

\usepackage{hyperref}
\usepackage{alltt}
\usepackage{amsmath}

\title{Attaching Translations to Proper Lexical Senses in DBnary}

\name{Andon Tchechmedjiev, Gilles Sérasset}

\address{ LIG-GETALP, Univ Grenoble Alpes\\
               BP 53 – 38051 Grenoble cedex 9 \\
               firstname.lastname@imag.fr\\}


\abstract{ 
The DBnary project aims at providing high quality Lexical Linked Data extracted from different Wiktionary language editions. Data from 10 different languages is  currently extracted for a total of over $3.16$M translation links that connect lexical entries from the 10 extracted languages, to entries in more than one thousand languages. 
In Wiktionary, glosses are often associated with translations to help users understand to what sense they refer to, wither though a textual definition or a target sense number. In this article we aim at the extraction of as much of this information as possible and then the disambiguation of the corresponding translations for all languages available. We use an adaptation of various textual and semantic similarity techniques based on partial or fuzzy gloss overlaps to disambiguate the translation relations (To account for the lack of normalization, e.g. lemmatization and PoS tagging) and then extract some of the sense number information present to build a gold standard so as to evaluate our disambiguation as well as tune and optimize the parameters of the similarity measures.
We obtain F-measures of the order of 80\% (on par with similar work on English only), across the three languages where we could generate a gold standard (French, Portuguese, Finnish) and show that most of the disambiguation errors are due to inconsistencies in Wiktionary itself that cannot be detected at the generation of DBNary (shifted sense numbers, inconsistent glosses, etc.). \\ \newline \Keywords{Wiktionary, Linked Open Data, Multilingual Resources}}


\DeclareMathOperator*{\argmax}{arg\!\max}
%\DeclareMathOperator*{\max}{\max}
\begin{document}

\maketitleabstract

\section{Introduction}

 Wiktionary is a lexical-semantic resource build collaboratively under the patronage of the Wikimedia Foundation (which also hosts the well known Wikipedia initiative). It is currently the biggest collaborative resource for lexical data. Wiktionary pages usually describe lexical entries by giving their part of speech, a set of definition, examples, lexico-semantic relations and many translations in more than a thousand target languages.

The DBnary project \cite{serasset:dbnary-swj} aims at providing high quality Lexical Linked Data extracted from different Wiktionary language editions. It currently extracts data from 10 different editions and gathers $3.16$M translation links relating lexical entries from the 10 extracted languages to entries in more than one thousand languages. These numbers are steadily growing as the DBnary data is extracted as soon as Wikimedia releases new dumps of the data (around once every 10-15 days for each language edition).

The source of these translation links are \emph{lexical entries}. The purpose of this work is to attach these translations to their correct \emph{word sense} and hence increasing the value of the DBnary data. Comparable efforts have been done (mainly on the UBY dataset), but these were limited to English and German. In this paper we worked on 9 language editions. Among these editions, we were faced with different habits from the different Wiktionary communities and with languages showing different linguistic properties. 

After detailing related works, we present the structure of the DBnary dataset. Then, after showing how we built an endogenous golden standard used to evaluate this work, we detail the methods used to achieve our purpose. Finally we evaluate our method and discuss the results.

\section{Related Work}

\subsection{Extracting Data from Wiktionary Language Editions.} Since its introduction in late 2002, Wiktionary has steadily raised in size (both with collaborative work and with automatic insertion of previously available free lexical data). Interest in Wiktionary as a source for lexical data for NLP applications has quickly raised and studies liked \cite{Zesch:AAAI2008} or \cite{navarro-EtAl:2009:PeoplesWeb} shown the richness and power of this resource. 

Since then, efforts has been specifically placed on the systematic extraction of Wiktionary data. Many of them have been done for a specific project and only represent a snapshot of Wiktionary. As all Wiktionary language editions evolve regularly (and independently) in the way they represents data, such effort are not suitable to prvide a sustainable access to Wiktionary data. 

Some efforts though are maintained and allow access over time. One of the most mature project is the \emph{JWKTL} API \cite{ZeschMuellerGurevych2008} giving access to  the English, German and Russian language editions. It is used in the UBY project \cite{gurevych2012uby} which provides an LMF based version of these editions. 

We should also mention the wikokit project \cite{krizhanovsky2010transformation} that provides access to the English and Russian editions and which has been used by \emph{JWKTL}.

\cite{HellmannSebastianandBrekleJonasandAuer} presents another attempt under the umbrella of the dbpedia project \cite{dbpedia-swj}, whose purpose is specifically to provide the Wiktionary data as Lexical Linked Open Data. The main interest of this approach is the collaborative approach used to create the extraction templates, which is the general approach of the dbpedia project. This project currently handles the English, French, Russian and German Wiktionary language editions.

This paper is part of the DBnary project \cite{serasset:dbnary-swj}, whose purpose is similar to \cite{HellmannSebastianandBrekleJonasandAuer}. Our specific goal is to provide LEMON structure lexical database that are structured as usual lexica. Indeed, we do extract data from Wiktionary, however, we currently restrict ourselves to the ``native'' data of each language edition, e.g. the French data is extracted from the French language edition and we disregard French data that is contained in other editions. At our knowledge, it is currently the most advanced extractor, as it currently actively supports 10 languages. It is also the only initiative that gives access to the whole history of the extracted data.

\subsection{Disambiguation the Source of Translations.} 

As far as the task of attaching translations to the proper word sense is concerned, the most similar work is described in \cite{meyer-gurevych:2012:PAPERS}. Its purpose was exactly the same, however, it only addressed German and English Language Editions. In this work, the gold standard to evaluate the method was manually created and was significantly smaller than the endogenous gold standard we extracted from the resource itself. This worked used a backoff strategy where string similarity measures were used only when other heuristics failed. These other heuristics also implied a finer analysis of definitions and glosses to distinguish between linguistic labels (stating the domain, register, tile, etc.).

In the work presented here, we achieve a similar scores on the languages we were able to evaluate with an endogenous gold standard, even though we only used string similarity measures and even if the evaluated languages show significantly different features (e.g. agglutinative aspect of the Finnish language).

\subsection{Similarity measures}
Our method is based on the application of gloss overlap measures and its extension with ideas taken from Hybrid textual similarity measures that match sentences both on the character and on the token level. In the work mentioned above \cite{meyer-gurevych:2012:PAPERS} a feature based similarity is used (gloss overlap), while in some of their prior work, \cite{MeyerGurevych:2010}  they use a textual similarity measure based on vector-spaces generated from corpora (Explicit Semantic Analysis).

In this work we propose using a very simple hybrid similarity measure by replacing the exact word match of the overlap calculation with an approximate string distance measure and using the more general framework of the Tversky measure that can be seen as a generalization of Lesk, the Dice coefficient , the Jaccard and Tatimono indexes.

 The idea proposed by \cite{Jimenez2010,Jimenez2012} is very similar in the sense that it exploits the Tversky index as a base and conjugates it with a textual similarity measure. That is, instead of incrementing the overlap count by 0 or 1, it is incremented by the value returned by the text similarity measure between the current pair of words being considered in the overlap calculation.
 
  Their text similarity measure is based on an empirically generated a q-gram model (character-grams corresponding to substrings) combined with point-wise mutual information weighting. However in this work, we want to minimize the requirement of generating similarity models. Doing so for 10 languages would require some effort and with future expansions to more languages, an impractically daunting task. 
 
 As such we chose to use a simple string distance measure for the approximate string match calculations. However, there are many such measure and it is necessary to select the right one for the task as will be detailed in Section \ref{sec:expe}. Moreover, there are existing so called ``Level 2'' or ``Hybrid'' similarity measures that already combine token overlap with token distance measures. Thus we will need to evaluate our proposed method with some of the existing methods so as to evaluate their viability. The various measures and a detailed performance comparison in a name matching task are presented by \cite{Cohen2003}.

\section{The DBnary Dataset}

DBnary is a Lexical Linked Open Dataset extracted from 10 Wiktionary language editions (English, Finnish, French, German, Greek, Italian, Japanese, Portuguese, Russian and Turkish). It is available on-line at \url{http://kaiko.getalp.org/about-dbnary}. At the time of writing, it contains more than 35 million triples. This number is steadily growing as the dataset evolves in parallel with the Wiktionary original data. Indeed, the dataset is automatically updated as soon as Wikimedia releases new Wiktionary dumps, i.e. every 10-15 days per language edition. 

DBnary is structured according the LEMON ontology for lexical linked data \cite{DBLP:conf/esws/McCraeSC11}. Table \ref{lemon-elts} shows the number of Lexical Elements, as defined in the LEMON ontologies, for the different extracted languages. 

The elements in DBnary that could not be represented with Lemon, were defined as a custom ontology built on top of existing Lemon classes and relations, most notably lexico-semantic relation and what we call \verb|Vocables|, the top level entries in Wiktionary, that correspond to Wiktionary pages for specific words, and that can contain several \verb|lemon:LexicalEntries| (lexemes) categorised in two levels:
\begin{enumerate}
	\item Homonymous distinction of words of different etymological origins (e.g. \verb|river [water stream]| v.s. \verb|river [one who rives or split])|
	\item For each etymological origin, the different lexico-grammatical categories (PoS) (e.g. \verb|cut#V| \verb|[I cut myself]| v.s. \verb|cut#Noun| \verb|[I want my cut of the winning]|)
\end{enumerate}

\subsection{Translation relations}

The DBnary dataset uses an ad-hoc representation of translation relations, as the LEMON model does not provide a vocabulary for such information. A \verb|Translation| is a RDF resource that gathers all extracted elements. As an example, one of the translation of the lexical entry \emph{frog} is represented as follows\footnote{In all dumps and in this paper, we use the Turtle syntax to represent RDF data.}:

\begin{small}
\begin{alltt}
eng:__tr_fra_1_frog__Noun__1
      a       dbnary:Translation ;
      dbnary:gloss "amphibian"@en ;
      dbnary:isTranslationOf
              eng:frog__Noun__1 ;
      dbnary:targetLanguage
              lexvo:fra ;
      dbnary:usage "f" ;
      dbnary:writtenForm "grenouille"@fr .
\end{alltt}
\end{small}

Properties of this resource point to the source \verb|LexicalEntry|, the language of the target (represented as a \verb|lexvo.org| entity \cite{deMeloWeikum2008c}), the target written form and, optionally, a gloss and usage notes.

The usage note gives information about the target of the translation (usually used to give the gender or a transcription of the target).

The gloss gives disambiguation information about the source of the translation. In the example given, it states that the given translation is valid for the word sense of \emph{frog} that may be described by the hint ``\emph{amphibian}''. Some of these glosses are textual and summarize or reprise the definition or part of the definition of one or more specific sense to which the translation specifically applies to.

As an example, the English \verb|LexicalEntry| \emph{frog} contains 8 word senses, defined as follows:

\begin{small}\begin{enumerate}
\item A small tailless amphibian of the order Anura that typically hops
\item The part of a violin bow (or that of other similar string instruments such as the viola, cello and contrabass) located at the end held by the player, to which the horsehair is attached
\item (Cockney rhyming slang) Road. Shorter, more common form of frog and toad
\item The depression in the upper face of a pressed or handmade clay brick
\item An organ on the bottom of a horse’s hoof that assists in the circulation of blood
\item The part of a railway switch or turnout where the running-rails cross (from the resemblance to the frog in a horse’s hoof)
\item An oblong cloak button, covered with netted thread, and fastening into a loop instead of a button hole.
\item The loop of the scabbard of a bayonet or sword. 
\end{enumerate}\end{small}

Translations of this entry are devided in 4 groups corresponding to the glosses ``\emph{amphibian}'', ``\emph{end of a string instrument’s bow}'', ``\emph{organ in a horse’s foot}'' and ``\emph{part of a railway}''. 

Additionally among the glosses, some may contain sense numbers, indicated by users in an ad-hoc way (may or may not be present, if the latter, no standard format is systematically followed or enforced). However, the presence of disambiguation information is very irregular and varies greatly between languages, both in terms of wiki structure and representation.

In the current state of the Wiktionary extraction process, we extract translation and when possible the associated glosses. However up to now, we did not exploit the information contained in the glosses to enrich and disambiguate the foreign language targets of translation relations.

As mentioned above, the information contained in translation glosses and their format are very variable between languages, both quantitatively and qualitatively. 

Indeed, as shown in Table \ref{lemon-elts} some language like Italian, contain no gloss altogether, others, like English attaches textual glosses to translations almost systematically, but with no sense numbers. Others still, like German hardly contain textual glosses but give sense numbers to translations. In other cases such as for Finnish, French and Portuguese, many translations have an attached (textual) gloss with associated sense numbers. 

In order to evaluate our method, we decided to use these mixed glosses that contain a textual hint and a sense number to create a endogenous gold standard.

\subsubsection{Creation of a gold standard}

It is often the case that among translation glosses that are available and that do contain textual information or sense numbers, there are many false positives and variability that result from the variety of structures employed in Wiktionary as well as artefacts resulting from the extraction process. Before we can proceed further, it is paramount to filter this information so as to keep only its relevant parts. 

More concretely tow steps must be followed of we are to successfully extract the information we need :
\begin{itemize}
   \item Remove empty glosses, or glosses containing irrelevant textual content that often correspond to TO DO notes in various forms (e.g. \emph{translations to be checked})
   \item Extract sense numbers from the glosses when available, using language dependent templates (e.g. ``\emph{textual gloss (1)}'' or ``\emph{1. textual gloss}'') 
\end{itemize}

\begin{table*}[htb]
\begin{center}\begin{footnotesize}
\begin{tabular}{lrrrrrrr}
\textbf{Language} & \textbf{Entries} & \textbf{LexicalSense} & \textbf{Translations} & \textbf{Glosses} & \textbf{Text} &  \textbf{Sense Num} & \textbf{Text+Sense Num.}\\
\hline
English & $544,338$ & $438,669$ & $1,317,545$ & $1,288,667$ & $1,288,667$ & $515$ & $515$ \\
Finnish & $49,620$ & $58,172$ & $121,278$ & $120,728$ & $120,329$ & $115,949$ & $115,550$ \\
French & $291,365$ & $379,224$ & $504,061$ & $136,319$ & $135,612$ & $28,821$ & $28,114$ \\
German & $205,977$ & $100,433$ & $388,630$ & $388,553$ & $3,101$ & $385,452$ & $0$ \\
Modern Greek & $242,349$ & $108,283$ & $56,638$ & $8,368$ & $8,368$ & $12$ & $12$ \\
Italian & $33,705$ & $47,102$ & $62,546$ & $0$ & $0$ & $0$ & $0$ \\
Japanese & $24,804$ & $28,763$ & $85,606$ & $22,322$ & $20,686$ & $4,148$ & $2,512$ \\
Portuguese & $45,109$ & $81,023$ & $267,048$ & $74,901$ & $72,339$ & $71,734$ & $69,172$ \\
Russian & $129,555$ & $106,374$ & $360,016$ & $151,100$ & $150,985$ & $115$ & $0$ \\
Turkish & $64,678$ & $91,071$ & $66,290$ & $53,348$ & $585$ & $52,901$ & $138$ \\
\hline
\end{tabular}
\caption{Number of elements in the current DBnary dataset, detailing the number of entries and word senses, along with the number of translations. The table also details the number of Glosses attach to translations, among which the amount of textual glosses, of glosses giving the sense identifier and, finally, the number of glosses that contain both a textual description and a word sense identifier.}
\label{lemon-elts}
\end{footnotesize}\end{center}
\end{table*}

When sufficient glosses contain both a textual hint and sense numbers, we removed the sense numbers\footnote{Translation are sometimes valid for several source word senses} from the gloss and used them to create a gold standard in trec\_eval format.

Now that we have successfully extracted as much of the information contained in translation glosses, we can move on to the disambiguation process itself. While, the steps above are indeed language specific, what follows was designed to be as generic and computationally efficient as possible, as we are required to periodically perform the disambiguation, whenever a new version of DBNary is extracted from the latest Wiktionary dumps.



\section{Attaching Translations to Word Senses}
\subsection{Formalization of translation disambiguation}
Let \(T\) be the set of all translation relations, \(L\) the set of all \verb|LexicalEntry| in a given language edition of DBNary. Let \(T_i\in T: Gloss(T_i)\) be a function that return the gloss of any translation \(T_i\in T\) and let \(Source(T_i)=L_{T_i}\) be a function that returns a reference to the source \verb|LexicalEntry|, \(L_{T_i}\) of a translation \(T_i\). Let \(Senses(L_i)=S_{L_i}\) be the set of all the senses associated with \verb|LexicalEntry| \(L_i\). Let \(S_{L_i}^k\) be the \(k\)-th sense contained in \(S_{L_i}\) and let \(Def(S_{L_i}^k)\) be a function that returns the textual definition of a sense \(S_{L_i}^k\). Finally let \(Sim(A,B)\) be a function that returns a semantic similarity or relatedness score between \(A\) and \(B\), where \(A,B\) are a pair of textual definitions or textual glosses. 

Then, we can express the disambiguation process as follows:
\[
\forall T_i \in T, S=Senses(Source(T_i)): 
\]
\[
Source^*(T_i) \leftarrow  \argmax_{S^k\in S} \{Score(Gloss(T_i),Def(S^k))\}
\]

This corresponds exactly to a standard semantic similarity maximisation and yields one disambiguated source sense per translation, however in many case a translation corresponds to one or more senses. The solution adopted by \cite{MeyerGurevych:oup2012} is to use a threshold \(k\) for their gloss overlap, however in our case, we want to be able to plug-in several different measures so as to find the most suitable one, as such just choosing a fixed arbitrary value for \(k\) is not readily and option. Thus, we need to add one more constraint: that the values returned by our similarity function need to be normalized between \(0 \mbox{ and  } 1\).

Here instead of taking a threshold \(k\), we set a window \(\delta\) around the best score in which the senses are accepted as a disambiguation of a given translation. We hypothesise that a relative threshold dependant on the maximal score, will set a precedent and be more representative of the possible range of values. Of course, setting a fixed threshold has for effect of not assigning any senses if all the scores are low, thus increasing precision at the cost of a lower recall. While in a general setting it is better to remove answers that are more likely to be mistakes as detecting errors \emph{a posteriori} is difficult. However in the context of the experiment, we prefer to keep such low or null scores as we will then be able with the help of the gold standard pin-point errors more precisely for the sake of our analysis.

We can express this formally by modifying the \(argmax\) function as such:
\[
\forall T_i \in T, S=Senses(Source(T_i)): 
\]
\[
M_S = \max_{S_k\in S}(Score((Gloss(T_i),Def(S^k))),
\]
\[
\argmax_{S_i\in S}^\delta \{Score(Gloss(T_i),Def(S^k))\}=
\]
\[
\{S^k\in S|  M_S > Score((Gloss(T_i),Def(S^k)) > M_S-\delta \}
\]

\subsection{Similarity Measure}
In order to disambiguate the translation, we need to be able to compute some form of semantic similarity measure. Given that the only information available in the translations is the gloss that summarises the definition of the corresponding sense, we need a measure to capture the similarity by comparing the translation glosses and the sense definitions. The Lesk \cite{citeulike:625530} measure is a standard semantic similarity measure, specifically suited for such tasks as it computes a similarity based on the number of exact overlapping words between definitions. The Lesk similarity's however has several important issues that need to be addressed when its use is mandated: 
\begin{itemize}
	\item If the sizes of the glosses or definitions is not the same, the Lesk measure will always favor longer definitions.
	\item The size and the appropriateness of the words contained in the definitions is important, as one key word to the meaning of the definition is missing (or the presence of a synonym for that matter) can lead to an incorrectly low similarity.
	\item The Lesk overlap is not in itself normalized, and the normalization process requires some though depending of the distinct problems at hand.
\end{itemize}
 
 The issues of normalization and of the unequal length of definitions are actually related, as one of the practical ways of doing so is to, for example, divide by the length of the shortest definition as a normalization. Moreover, one can only notice the similarity between the Lesk measure and other overlap coefficients such as the dice coefficient, the Jaccard or Tatimono indices. In fact, all of these measures are special forms of the Tversky index, which stems from the works on similarity of cognitive psychologist A. Tversky \cite{tversky77similarity}.

The Tversky index can be defined as follows. Let \(s_1 \in Senses(L_1)\)  and \(s_2 \in Senses(L_2)\) be the senses of two lexical entries \(L_1\) and \(L_2\). Let \(d_i=Def(s_i)\) be the definition of \(s_i\), represented as a set of words. The similarity between the senses \(Score(s_1, s_2)\) can be expressed as 
\[
Score(s_1,s_2) = 
\frac{|d_1\cap d_2|}{|d_1\cap d_2| + \alpha |d_1-d_2| + \beta |d_2-d_1|}
\]

The measure can further be generalized following \cite{DBLP:conf/otm/PirroE10} by replacing the cardinality function by any function \(F\). Depending on the values of \(\alpha\) and \(\beta\), the Tversky index takes the particular form of other similar indexes. For \((\alpha=\beta=0.5)\) for example it is equivalent to the dice coefficient, and for  \((\alpha=\beta=1)\) to the Tatimono index. More generally, the values of \(\alpha\) and \(\beta\) express how much emphasis one wants to attribute to the commonality or differences of one or the other set.

 The Tversky index in itself is not a metric in the mathematical sense, as it is neither symmetric nor respects the triangular inequality, however, a symmetric variant has been proposed by \cite{Jimenez2010} for such cases where the symmetry property is important or required.
% : 
% 
% \[
% S(s_1,s_2) = \frac{|d_1\cap d_2|}{|d_1\cap d_2| + \beta (\alpha a + (1-\alpha)b)}\]
% \[
% a=min(|d_1\cap d_2|,|d_2\cap d_1|)\\
% \]\[
% b=max(|d_1\cap d_2|,|d_2\cap d_1|)\\ 
% \]
 
 When making use of the Tversky index or its adapted symmetric version, careful attention must be given to the weights that one might use depending on the application. 
 
 In this paper, there is no specific indication that there is any need for a symmetric version, however, we shall evaluate if there is any real benefit for the task we have at hand in Section \ref{sec:expe}. 

\subsubsection{Multilingual Setting \& Partial overlaps}
 When working on a single language such as English and French, we have at our disposal tools such as a lemmatizer or a stemmer that may help to retrieve a canonical representation of the terms. Thus, we can hope to maximize the overlap and reduce the usual sparsity of glosses or sense definitions. For agglutinative languages like German or Finnish, highly inflective language (for example in the Bangla language, common stems are often composed of a single character, which makes stemming difficult to exploit) or languages with no clear segmentation, the preprocessing steps are paramount in order to make overlap based measures viable. If one is working on a single language, even if stemmers and lemmatizers do not exist, it is not impossible to build such a tool.

However, in the context of this work we are currently dealing with 10 languages (and potentially in the future with all the languages present in Wiktionary) and thus, in order to propose a truly general method, we cannot expect the prerequisite presence of such tools. 

How then, can we manage to compute overlaps effectively? When computing Lesk, if two words overlap, the score is increased by 1 and of two words do not overlap, the overlap does not change. What if we had a way to count meaningful partial overlaps between words and instead of adding 1, we may add whatever values between 0 and 1 represents the amount of overlap.

The simplest approach to the problem is to use some form of partial string matching metric to compute partial overlaps, a seemingly trivial and classical approach that can, however, greatly improve the result as we shall endeavour to elicit and as has already been extensively shown by \cite{Jimenez2012}. 

As mentioned in the Related Work section, there are many approximate string matching measures as reviewed by \cite{Cohen2003}. We integrate these measures in the Teversky index by setting the \(F\) function that replaces the set cardinality function appropriately (a simplified version of soft cardinality):

\[
	A \mbox{ , a set} : F(A) = (\sum_{A_i,A_j \in A}sim(A_i, A_j))^{-1}
\]

In our case, \(sim\) will be an string distance measure.

\subsubsection{Longest Common Substring Constraints}
With this similarity measure, we are mainly interested in capturing word that have common stems, without the need for a stemmer, as such we do not for example want to consider the overlap of prefixes or suffices, as they do not carry the main semantic information of the word. If two words only match by a common suffix that happens to be used very often in that particular language, we will have a non-zero overlap, but we would have captures no sematic information whatsoever. Thus, in this word we put a threshold of a longest common subsequence of three characters.

\section{Experiments}
\label{sec:expe}
As was mentioned previously, we have been able to extract gold standards from the sense numbered textual glosses of translations in certain language editions of Wiktionary. Then we stripped all sense number information from the glosses, so we could disambiguate those same translation and then evaluate the results on the previously generated gold standard.

We will first describe how we generated the Gold standard and the tools and measures used for the evaluation. We will then proceed onto the empirical selection of the best parameters for our Tversky index as well as the most appropriate string distance measure to use for the fuzzy or soft cardinality. Then, we will compare the results of the optimal Tversky index with other Level 2 similarity measures.
 
\subsection{Evaluation}
Let us first describe the Gold Standard generation process, then proceed on to describing how we represented the Gold Standard in Trec\_eval format, a scorer program from the query answering Trec\_Eval campaign. Finally we will described the evaluation measures we will use. 
\subsection{Gold Standard}
Only certain languages meet the requirements for the generation of a sufficiently large Gold Standard. To be more specific, we could only chose among languages that where:
\begin{enumerate}
	\item There are textual glosses (for the overlap measures)
	\item There are numbers in said glosses indicating the right sense number
	\item The above are available in a sufficient quantity (at least a few thousand)
\end{enumerate}

Four languages could potentially meet the criteria (see the last column of Table \ref{lemon-elts}): French, Portuguese, Finnish and Japanese, however we could only manage the time to extract gold standards for French, Portuguese and Finnish.

\subsubsection{Trec\_eval, scoring as a query answering task}

A query answering task is more generally a multiple-labelling problem, which is exactly equivalent to what we are producing when we use the threshold \(\delta\). Here, we can consider that each translation number is the query identifier and that each sense URI is a document identifier. We answer the "translation" queries by providing one or more senses and an associated weight.

Thus, we can generate the gold standard and the results in the Trec\_eval format, the very complete scorer for and information retrieval evaluation campaign of the same name.

\subsubsection{Measures}
We will use the standard set matching metrics used in Information Retrival and Word Sense Disambiguation, namely Recall, Precision and F\_{1} measure. Where, \(P=\frac{|\{Relevant\}\cap\{Disambiguated\}|}{|\{Disambiguated\}|}\), \(R=\frac{|\{Relevant\}\cap\{Disambiguated\}|}{|\{Relevant\}|}\), and \(F_1 = \frac{2\cdot P \cdot R}{P + R} \), the harmonic mean of \(R\) and \(P\). However, for the first step consisting in the estimation of the optimal parameters, we will only provide the \(F_1 score\), as we are interested in maximising both recall and precision in an equal fashion.
\subsection{Similarity Measure Tuning}
There are several parameters to set in our Tversky index, however the first step is to find the most suitable string distance measure.

\subsubsection{Optimal String Distance Metric}
 The \(\delta\) parameter influences performance independently of the similarity measure, so we can first operate with \(\delta=0\), which restrincts us to a single disambiguation per translation. Furthermore, the weights of the Tvsersky index are applied downstream from the string edit distance, and thus does not influence the relative performance of the different string distance metrics combined to our  Tversky index. Thus for this first experiment, we will set \(\alpha=\beta=0.5\), in other words the index becomes the dice coefficient.

As for the selection of the string similarity measures to compare, we take the best performing measures from \cite{Cohen2003}, namely Jaro-Winkler, Monge-Elkan, Scaled Levenshtein Distance, to which we also add the longest common substring for reference. As a baseline measure, we will use the Tversky index with a standard overlap cardinality.

We give the following short notations for the measures: Tversky Index -- Ts; Jaro-Winkler -- JW; Monge-Elkan -- ME; Scaled Levenshtein -- Ls; Longest Common Substring -- Lcss; F -- Fuzzy. For example standard Tversky index with classical cardinality shall be referred to as "Ti", while the fuzzy cardinality version with a Monge-Elkan string distance shall be referred to as "FTiME". 

Table \ref{tab:expe1} presents the results for each string similarity measure and each of the three languages (French, Finnish, Portuguese).

As we can see, for all language, the best string similarity measure is clearly the scaled Levenstein measure as it systematically exhibits a score higher from \(+1\%\) to \(+1.96\%\).

\begin{table}
{\centering \footnotesize
\begin{tabular}{|c|c|c|c|}
\hline &French&Portuguese&Finnish\\
\hline &F1&F1&F1\\
\hline FTiJW&0.7853&0.8079&0.9479\\
\hline FTiLcss&0.7778&0.7697&0.9495\\
\hline FTiLs&\textbf{0.7861}&\textbf{0.8176}&\textbf{0.9536}\\
\hline FTiME&0.7684&0.7683&0.9495\\
\hline Ti&0.7088&0.7171&0.8806\\
\hline 
\end{tabular} 
\caption{Results comparing the performance in terms of F\_1 score for French, Finnish and Portuguese (ighest scores in bold).}
\label{tab:expe1}
}
\end{table}

\subsubsection{Optimal \(\alpha\), \(\beta\) selection}
Now that we have found the optimal string distance measure, we can look for the optimal ratio of \(\alpha\) and \(\beta\). Here, we will keep both values complementary, that is \(\alpha=1-\beta\) so as to obtain more balances score that remain in the 0 to 1 range. 

Given that translation glosses are very short (often a single word), it is very likely that the optimum is around \(\alpha=1-\beta=0.1\), given that what interests us is specifically that the word in the translation gloss matches with less importance for the remaining words in the sense definition that do not match.

We chose, here, to evaluate the values of \(\alpha\) and \(\beta\) in steps of \(0.1\). Figure \ref{fig.1} graphically shows the \(F\_1\) score for each pair of values of \(alpha\) and \(beta\) for all three languages. We can indeed confirm our hypothesis as the optimal value in all three cases is indeed \(\alpha=1-\beta=0.1\) with a difference between \(+0.15\%\) to \(+0.43\%\) with the second best scores.
	
\begin{figure}\centering
\includegraphics[width=0.66\columnwidth]{alphabetafig}
\caption{F1 score for Finnish, French and Portuguese depending on the value of \(\alpha\) and \(\beta\).}
\label{fig.1}
\end{figure}

\subsubsection{Optimal \(\delta\) selection}
Now that we have fixed the best values of \(\alpha\) and \(\beta\) we can search for the best value for \(\delta\). We make delta vary in steps of \(0.05\) between \(0\) and \(0.3\). The choice of the upper bound is based on the hypothesis that the optimal value is somewhere closer to 0, as a too large threshold essentially means that most of the senses for each translation might be considered as corresponding to the translation at hand and thus it should drastically reduce performance. 

The \(\delta\) heuristic affects the results of the disambiguation whether the measure is  Tversky index or another Level 2 Textual similarity. Thus, in this experiment, we will also include Level 2 version of the three string distance measures that we used in the first experiment.

Figure \ref{fig.2} graphically presents the \(F_1\) scores for each value of \(delta\) and each language. The first apparent trend is that Level 2 measures are systemically performing much worse (by up to 30\%) than our own similarity measure. Depending on the language different values of \(delta\) are optimal, even though it is difficult to see a great difference. For French \(\delta=0.10\), for Finnish \(\delta=0.15\)
 and for Portuguese \(\delta=0.10\). 
In all three previous experiments, it became apparent, that the same string similarity measure, the same values for alpha and beta as well as the same value for delta were optimal, which leads us to believe that their optimality will be conserved across all languages. However, especially for the string similarity measure, it is reasonable to believe that for languages such a Chinese or Japanese that lack segmentation, the optimal choice for the string distance measure may be entirely different.
\subsection{Final Disambigation Results}

Now that we have found all the optimal parameters, we can actually present the final results combining all the optimal parameters. They are in fact the very results that were optimal in the previous experiment. We shall now take these results and place them in a separate table (Table \ref{tab:final}) so as to make the comparison analysis easier. We will use the chance of random selection as well as the most frequent sense selection as baseline for this comparison.

The first thing one can notice is that there is a stark difference between the scores of Finnish, and the rest. Indeed, first of all the random baseline and most frequent sense baselines are an indication that the French and Portuguese DBNaries are highly polysemous, while Finnish contains a very large amount of monosemous entries, which artificially inflates the value of the score. 

Another point of interest is that the random baseline is higher (up to 6.6\%) than the most frequent sense baseline, which indicates that the first sense if often not the right sense to select to match the translation. 

We can see that for all three languages we achieve a good performance compared to what is presented in the literature, most notably in the fact that most of the errors, can easily be identified as such just by looking at whether or not it produced any overlap.

\begin{table}
{\centering \footnotesize
\begin{tabular}{|c|c|c|c|c|c|}
\hline &P&R&F1&MFS F1&Random\\
\hline Portuguese&0.8572&0.8814&0.8651&0.2397&0.3103\\
\hline Finnish&0.9642&0.9777&0.9687&0.7218&0.7962\\
\hline French&0.8267&0.8313&0.8263&0.3542&0.3767\\
\hline 
\end{tabular}
\caption{Final results with optimal measure and parameter values. Precision, Recall, F1 measure for all three languages compared against the MFS and Random Baselines.}
\label{tab:final}
}
\end{table} 

\begin{figure}
\centering
\includegraphics[width=.68\columnwidth]{french}
\includegraphics[width=.68\columnwidth]{portuguese}
\includegraphics[width=.68\columnwidth]{finnish}
\caption{Graphical representation of the F1 score against the  value of delta for our measure and other Level 2 Measures}
\label{fig.2}
\end{figure}

\subsection{Error analysis}

We did not here perform a full fledged and systematic error analysis, but rather an informal manual sampling so as to have an idea of what the error can be and if there are ways to correct them by adapting the measures or the methodology.
We looked at some of the error made by the disambiguation process and manually checked them so as to categorize them. We found three main categories:
\begin{enumerate}
\item No overlap between the gloss and sense definitions (Random choice by our algorithm), this happens when the translation gloss is a paraphrase of the sense definition or simply a metaphor for it.
\item The overlap is with the domain category label or the example glosses, which we do not currently extract. This is a particular case of the first type of error.
\item New senses have been introduced in Wiktionary and shifted sense numbers, which were not subsequently updated in the resource. Such errors cannot be detected during the extraction process.
\end{enumerate}

We can in fact easily find all the errors due to the lack of overlap and correct the errors of type 2 by enriching the extraction process of DBNary. Thus we can single out errors that are due to inconsistencies in the resource and thus potentially use the disambiguation results to indicate to users where errors are located an need to be updated.

\section{Conclusion}

With our method, we were able to determine and optimal similarity measure for disambiguating translation in DBNary. Similar results across the three evaluation languages suggests that it is a general optimality that can be applied to all the languages currently present in DBNary, although for Asian Languages that have no segmentation, it is likely not the case.

The we compared the results and concluded that our method is viable for the task of disambiguating glossed translation relations, especially considering the low random baselines and first sense baselines compared to the top score of our disambiguation method.

For translation relations without glosses, the disambiguation process is more complex and is part of the Future Work that we plan on carrying out.

\section{Acknowledgements}

Place holder.

%\nocite{*}

\bibliographystyle{lrec2006}
\bibliography{dbnary-wsd}

\end{document}

\begin{figure}[h]
\begin{center}
%\fbox{\parbox{6cm}{
%This is a figure with a caption.}}
\includegraphics[scale=0.5]{image1.eps} 
\caption{The caption of the figure.}
\label{fig.1}
\end{center}
\end{figure}

\section{Footnotes}

Footnotes are indicated within the text by a number in superscript\footnote{They should be in Times 9, and appear at the bottom of the same page as their corresponding number. Footnotes should also be separated from the rest of the text by a horizontal line 5 cm long.}.

\section{Copyrights}

The Lan\-gua\-ge Re\-sour\-ce and Evalua\-tion Con\-fe\-rence (LREC) proceedings are published by the European Language Resources Association (ELRA). They include different media that may be used (i.e. hardcopy, CD-ROM, Internet-based/Web, etc.).

ELRA's policy is to acquire copyright for all LREC contributions. In assigning your copyright, you are not forfeiting your right to use your contribution elsewhere. This you may do without seeking permission and is subject only to normal acknowledgement to the LREC proceedings.
