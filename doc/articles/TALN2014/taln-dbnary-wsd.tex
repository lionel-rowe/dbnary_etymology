%% Exemple de source LaTeX pour un article soumis à TALN
\documentclass[10pt,a4paper,twoside]{article}

\usepackage{times}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}

\usepackage{hyperref}
\usepackage{alltt}
\usepackage{amsmath}


% faire les \usepackage dont vous avez besoin AVANT le \usepackage{taln2014} 

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% 
\usepackage{taln2014}
\usepackage[frenchb]{babel}
%
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %


% Titre complet
\title{Modèle de document pour TALN 2014}

\author{Untel Trucmuche\up{1, 2}\quad Unetelle Machinchose\up{1, 3}\\
  (1) LPL, AMU, CNRS, 5 avenue Pasteur, 13100 Aix-en-Provence \\ 
  (2) LIF, AMU, CNRS, 163 avenue de Luminy, 13288 Marseille Cedex 9\\ 
  (3) Lab, adresse, CP Ville, Pays \\ 
  utrucmuche@lpl-aix.fr, umachinchose@adresse-academique.fr \\ 
}

% Titre qui apparait en en-tête (1 ligne maxi)
\fancyhead[CO]{Modèle de document pour TALN 2014} 
% Auteurs qui apparaissent en en-tête (1 ligne maxi)
\fancyhead[CE]{Untel Trucmuche, Unetelle Machinchose} 

\DeclareMathOperator*{\argmax}{arg\!\max}

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

\begin{document}

\maketitle


\resume{
Ici, un résumé en français (max. 150 mots).
}
\\

\abstract{
The DBnary project aims at providing high quality Lexical Linked Data extracted from different Wiktionary language editions. Data from 10 different languages is  currently extracted for a total of over $3.16$M translation links that connect lexical entries from the 10 extracted languages, to entries in more than one thousand languages. 
In Wiktionary, glosses are often associated with translations to help users understand to what sense they refer to, wether through a textual definition or a target sense number. In this article we aim at the extraction of as much of this information as possible and then the disambiguation of the corresponding translations for all languages available. We use an adaptation of various textual and semantic similarity techniques based on partial or fuzzy gloss overlaps to disambiguate the translation relations (To account for the lack of normalization, e.g. lemmatization and PoS tagging) and then extract some of the sense number information present to build a gold standard so as to evaluate our disambiguation as well as tune and optimize the parameters of the similarity measures.
We obtain F-measures of the order of 80\% (on par with similar work on English only), across the three languages where we could generate a gold standard (French, Portuguese, Finnish) and show that most of the disambiguation errors are due to inconsistencies in Wiktionary itself that cannot be detected at the generation of DBnary (shifted sense numbers, inconsistent glosses, etc.).
}
\\

\motsClefs{Ici une liste de mots-clés en français}
{Wiktionary, Linked Open Data, Multilingual Resources}


%%================================================================
\section{Introduction}

 Wiktionary est un ressource lexico-sémantique construite collaborativement sous l'égide de la Fondation Wikimedia (qui héberge également la célèbre initiative Wikipedia). C'est actuellement la ressource collaborative de données lexicales la plus grande. Les pages Wiktionary décrivent habituellement des entrées lexicales en donnant leut catégorie grammaticale, un ensemble de définitions, des exemples, des relation lexico-sémantiques ainsi que des traductions dans plus de mille langues cible.

Le projet DBNary \cite{serasset:dbnary-swj} a pour objectif de fournir des données liées lexicales de haute qualité extraites des éditions en différentes langues de Wiktionary. DBNary permet actuellement d'extraire des données issues de 10 éditions et regroupe $3,16$M de liens de traduction qui mettent en relation les entrées lexicales des 10 langues extraites vers des entrées dans plus de mille langues. Ces chiffres sont en augmentation constante, sachant que le jeu de données DBNary et extrait dès que Wikimedia met à disposition de nouvelles vidanges\textbf{???} des données (environs tous les 10 à 15 jours pour chaque langue).

La source de ces liens de traduction sont des \emph{entrées lexicales}. Le but de ce travail est d'attacher ces traductions au sens de mot correct correspondant et d'ainsi augmenter la valeur et la qualité de DBNary.
Des travaux similaires ont été menés (principalement dans le jeu de données Uby), mais sont limités à l'anglais et l'allemand. Dans cet article, nous avons travaillés des éditions dans 9 langues, avec lesquelles nous avons du faire face avec les habitudes diverses des différentes communautés Wiktionary qui s'exprimaient par différentes propriétés linguistiques mises en avant. 

Après une revue des travaux similaires, nous présentons la structure de DBNary. Ensuite, après avoir montré comment nous construisons l'étalon-or endogène  que nous utilisons pour évaluer notre travail, nous détaillons les méthodes employées pour atteindre notre but. Enfin, nous évaluons notre méthode est interprétons les résultats.


\section{Travaux Similaires}

\subsection{Extraction de données depuis les éditions de langue Wiktionary} Depuis sa création en 2002, Wiktionary à connu une augmentation régulière en taille (à la fois par un travail collaboratif ainsi que l'insertion automatique de données lexicales libres précédemment disponible). L'engouement pour Wiktionary en tant que source pour la production de données lexicales pour des applications du TAL c'est développé rapidement , et des études comme par exemple \cite{Zesch:AAAI2008} où \cite{navarro-EtAl:2009:PeoplesWeb} démontrent la richesse et la puissance de ces ressources.

Depuis, les travaux se sont surtout concentrés sur l'extraction systématique de données provenant de Wiktionary.  Ls plus part comme ressource spécifique à un projet donné, et qui ne constituent ainsi qu'une capture de Wiktionary figée dans le temps. Sachant que toutes les éditions de Wiktionary évoluent régulièrement (et indépendamment) vis-à-vis de comment sont représentées leur données, de tels travaux ne peuvent pas fournir un accès durable aux données de Wiktionary.


Certains des travaux cependant dont également maintenus et permettent un accès au cours du temps. L'un de plus mature de ces travaux est l'API \emph{JWKTL} \cite{ZeschMuellerGurevych2008} qui donne accès aiux éditions Anglaises, Allemandes et Russes. C'est cette dernière qui est utilisée dans le projet UBY \cite{gurevych2012uby}, qui met à disposition une version LMF de ces éditions.

Il faut également faire mention du projet wikokit \cite{krizhanovsky2010transformation}, qui donne accès  aux éditions Anglaises et Allemandes, et qui à été utilisée par \emph{JWKTL}.

\cite{HellmannSebastianandBrekleJonasandAuer} présentent un autre tentative, sous l'égide du projet dbpedia  \cite{dbpedia-swj}, dont l'objectif est en particulier de fournir un accès aux données de Wiktionary en tant que données liées ouvertes. La raison principale que rend cette approche intéressante, est l'aspect collaboratif, utilisé pour créer les patrons d'extraction, ce qui correspond à l'approche générale du projet dbpedia. Ce projet donne accès aux éditions de Wiktionary Anglais, Française, Russe et Allemande.ditions.

Cet article et les travaux effectués ici le sont dans le cadre du projet DBnary \cite{serasset:dbnary-swj}, dont l'objectif est similaire à celui de \cite{HellmannSebastianandBrekleJonasandAuer}. Plus précisément, notre objectif est de fournir une base de donnée lexicale au format LEMON, qui structure les données comme dans des lexiques traditionnels. En effet, nous extrayons des données issues de Wiktionary, en nous restreignant toutefois aux donnée ''natives'' de chaque édition. Par exemple, nous extrayons les données en Français de l'édition Française, mais ignorons les données en Français contenues dans d'autre éditions.
À notre connaissance, DBNary est actuellement l'extracteur pour Wiktionary le plus avancé, avec sont support actif courant de 12 langues. C'est également le seul projet qui donne accès à tous l'historique des données extraites. 

\subsection{Désambiguïsation des sources des liens de traduction} 

En ce qui concerne le rattachement des liens de traduction aux sens de mots les plus adéquats (déambiguïsation des liens de traduction), les travaux les plus similaires sont ceux de \cite{meyer-gurevych:2012:PAPERS}, dont les objectifs correspondent aux nôtres. Cependant leurs travaux ne portent que sur les éditions Anglaises et Allemandes. De plus, l'étalon-or utilisé pour évaluer leur méthode  fut créé manuellement et est d'une taille significativement plus petite que l'étalon-or endogène que nous avons ici extrait de la ressource elle-même.  Leur travail utilise également une stratégie de repli (vers le sens le plus fréquent), quand leur heuristiques à base de mesures de similarité ainsi que basées sur la structure de la ressource échouent.  Les autres heuristiques qu'ils utilisèrent, impliquent également une analyse plus fine des définissions et gloses afin de notamment faire une distinction entre les étiquettes  linguistiques (domaine, registre, titre, etc.).

Dans le travail ici présent, nous atteignons des résultats similaires sur les langues où nous avons la possibilité d'évaluer la désambiguïsation avec un étalon-or endogène, malgré le fait que nous n'aillons uniquement utilisés des mesures de similarité de chaines et de mots, et ce même dans les langues avec des propriétés moins communes (par exemple la nature agglutinante du Finnois.)  

\subsection{Mesures de similarité}
Notre méthode est basée sur l'application de mesures d'intersection de gloses et de leurs extensions avec des idées provenant des mesures de similarité textuelles hybrides, qui calculent une correspondance de sous-séquences à la fois au niveau du caractère et du mot. Dans les travaux cités ci-dessus, \cite{meyer-gurevych:2012:PAPERS}, une mesure de similarité à base de traits est utilisée (intersection de gloses), alors que dans leurs travaux antérieurs \cite{MeyerGurevych:2010}, ils ont utilisés une mesure de similarité textuelle basée sur des espaces vectoriels générés à partir de corpus (Analyse Sémantique Explicite).

Dans notre travail, nous proposons l'utilisation d'une mesure de similarité simple, où nous remplaçons la correspondance de mots exacte du calcul d'intersection par une mesure de distance de chaine approchée, et en nous plaçant dans le contexte plus général de la mesure de similarité qu'est l'indice de Tversky (qui peut être vu comme une généralisation de Lesk, du coefficient de Dice, des indices de Jaccard et Tatimono, etc.)

 L'idée de \emph{cardinalité molle}(``soft-cardinality'') proposée par \cite{Jimenez2010,Jimenez2012} est très similaire, dans le sens où elle exploite l'index de Tversky comme base et la conjugue avec une mesure de similarité textuelle. C'est à dire, au lieu d'incrémenter la cardinalité de l'intersection de 0 où 1, elle est incrémentée par la valeur retournée par une mesure de similarité de chaine pour chaque paire de mots considérée lors du calcul de la cardinalité de l'intersection. 
 
  Leur mesure est basée sur la notion de q-grammes, générés empiriquement (caractère-grammes correspondant à des sous-chaines) avec une pondération à base de contenue d'information mutuel ponctuel (pointwise mutual information). Dans note cas, construire ce type de modèles pour 12 langues nécessiterais de nombreux efforts, et avec l'extension future à plus de langues (voire toutes), cela deviendrait une tâche pratiquement impossible. 
 
 De ce fait, nous avons choisis une mesure de distance de chaine simple pour le calcul de la correspondance partielle de chaines. Cependant, il y a de nombreuses mesures disponibles et il advient de choisir celle qui est la plus appropriée pour notre tâche (voir Section \ref{sec:expe}). Qui plus est, il existe également des mesures dites de ``Niveau 2'' qui combinent déjà de différentes manières des mesures de similarité de chaines avec des mesure d'intersection de termes. Ainsi il faudra évaluer la méthode que nous proposons, avec certaines de ces mesures de ``Niveau 2'' existantes afin d'estimer le viabilité. Toutes ces mesures ont fait l' objet d'une évaluation et d'une comparaison extensive entre elles dans le contexte d'une tâche de correspondance de noms \cite{Cohen2003}.

\section{Le jeu de données DBNary}

DBnary est un jeu de données liées ouvertes extraites depuis 12 éditions de langues Wiktionary (Anglais, Finnois, Français, Allemand, Grec, Italien, Japonais, Portugais, Russe, Turque, Espagnol, Bulgare). La ressource est disponible en ligne à l'adresse \url{http://kaiko.getalp.org/about-DBnary}. Au moment de l'écriture, DBnary contiens  plus de  35 millions de triples. Ce nombre à constamment augmenté au long de l'évolution du jeu de données au rythme des évolutions des données originales de Wiktionary. En effet, DBnary est automatiquement mis-à-jour dès que Wikimedia met à disposition une nouvelle version des ``vidanges'' Wiktionary, c'est-à-dire à peu près tous les 10 à 15 jours.

DBNary est structuré en suivant le modèle de l'Ontologie LEMON pour la représentation de données lexicales liées \cite{DBLP:conf/esws/McCraeSC11}. Le tableau \ref{lemon-elts} donne une idée du nombre d'éléments lexicaux telles que définies dans l'ontologie LEMON dans les différents éditions de langues.

Les éléments dans DBnary qui n'ont pu être représentés en LEMON, on été définis dans une ontologie sur mesure construite sur la base des classes et relations LEMON, notablement les relations lexico-sémantiques ainsi que ce que nous appelons des \verb|Vocables|, les entrées de haut-niveau dans Wiktionary qui correspondent aux pages Wiktionary pour des mots spécifiques et qui contiennent plusieurs entrées lexicales (\verb|LexicalEntry|) catégorisées en deux niveaux:
\begin{enumerate}
	\item Distinction de mots homonymes selon l'origine étymologique (par exemple: \verb|mode [la mode actuelle]|) contre \verb|mode [le mode de fonctionnement]|.
	\item Pour chaque origine étymologique différente, distinction selon la catégorie grammaticale (par exemple \\ \verb|rouge#Adj [la voiture rouge]| contre \verb|rouge#Nom [le rouge au front]|)
\end{enumerate}

\begin{table*}[htb!]
\begin{center}\begin{footnotesize}
\begin{tabular}{lrrrrrrr}
\textbf{Langue} & \textbf{Entrées} & \textbf{LexicalSense} & \textbf{Traductions} & \textbf{Gloses} & \textbf{Texte} &  \textbf{Nbr. de sens} & \textbf{Texte + Nbr. de sens }\\
\hline
Anglais & $544,338$ & $438,669$ & $1,317,545$ & $1,288,667$ & $1,288,667$ & $515$ & $515$ \\
Bulgare & $13888$ & $10104$ & $10104$ & $0$ & $0$ \\
Espagnol & $114951$ & $66931$ & $2786$ & $64145$ & $0$ \\
Finnois & $49,620$ & $58,172$ & $121,278$ & $120,728$ & $120,329$ & $115,949$ & $115,550$ \\
Français & $291,365$ & $379,224$ & $504,061$ & $136,319$ & $135,612$ & $28,821$ & $28,114$ \\
Allemand & $205,977$ & $100,433$ & $388,630$ & $388,553$ & $3,101$ & $385,452$ & $0$ \\
Grec Moderne & $242,349$ & $108,283$ & $56,638$ & $8,368$ & $8,368$ & $12$ & $12$ \\
Italien & $33,705$ & $47,102$ & $62,546$ & $0$ & $0$ & $0$ & $0$ \\
Japonais & $24,804$ & $28,763$ & $85,606$ & $22,322$ & $20,686$ & $4,148$ & $2,512$ \\
Portugais & $45,109$ & $81,023$ & $267,048$ & $74,901$ & $72,339$ & $71,734$ & $69,172$ \\
Russe & $129,555$ & $106,374$ & $360,016$ & $151,100$ & $150,985$ & $115$ & $0$ \\
Turque & $64,678$ & $91,071$ & $66,290$ & $53,348$ & $585$ & $52,901$ & $138$ \\

\hline
\end{tabular}
\caption{Nombre d'éléments dans le jeu de données DBnary actuel avec des détails sur le nombre d'entrées et de sens de mot ainsi que le nombre de traduction. Le tableau détaille également le nombre de gloses rattachées à des traduction, et de manière plus précise le nombre de gloses textuelles, le nombre de gloses qui contiennent un numéro de sens et enfin le nombre de gloses qui contiennent à la fois une description textuelle et un numéro de sens.}
\label{lemon-elts}
\end{footnotesize}\end{center}
\end{table*}

\subsection{Relations de traduction}

DBnary utilise une représentation ad-hoc pour les relations de traduction, sachant que le modèle LEMON ne propose pas de vocabulaire pour représenter une telle information.  \verb|Translation| est une ressource RDF qui rassemble toute l'information se rapportant aux relations de traduction. Par exemple, l'une des traductions de l'entrée lexicale de \emph{frog} est représentée comme suit : \footnote{Dans cet article ainsi que pour les jeux de données DBNary nous utilisons la syntaxe RDF Turtle.}:

\begin{small}
\begin{alltt}
eng:__tr_fra_1_frog__Noun__1
      a       dbnary:Translation ;
      dbnary:gloss "amphibian"@en ;
      dbnary:isTranslationOf
              eng:frog__Noun__1 ;
      dbnary:targetLanguage
              lexvo:fra ;
      dbnary:usage "f" ;
      dbnary:writtenForm "grenouille"@fr .
\end{alltt}
\end{small}

Les propriétés de cette ressource pointent vers une source de type \verb|LexicalEntry|, la langue de la cible (représentée comme une entrée \verb|lexvo.org| \cite{deMeloWeikum2008c}), la forme de surface de la cible, et 'éventuellement une glose et des notes d'usage. 
Les notes d'usage donnent des informations sur la cible de la traduction (habituellement le genre où la transcription de la cible).

La glose donne une information de désambiguïsation sur la source de la traduction. Dans l'exemple donné, il est dit que la traduction ne correspond qu'au sens de \emph{frog} qui peut être décrit par l'indice ``\emph{amphibian}''. Certaines de ces gloses sont textuelles et résument ou reprennent la définition ou une partie de celle-ci d'une ou plusieurs sens de la source auxquels s'applique la traduction.

Par exemple , la \verb|LexicalEntry| Anglaise \emph{frog} contient huit sens de mots définis comme suit : 

\begin{small}\begin{enumerate}
\item A small tailless amphibian of the order Anura that typically hops
\item The part of a violin bow (or that of other similar string instruments such as the viola, cello and contrabass) located at the end held by the player, to which the horsehair is attached
\item (Cockney rhyming slang) Road. Shorter, more common form of frog and toad
\item The depression in the upper face of a pressed or handmade clay brick
\item An organ on the bottom of a horse’s hoof that assists in the circulation of blood
\item The part of a railway switch or turnout where the running-rails cross (from the resemblance to the frog in a horse’s hoof)
\item An oblong cloak button, covered with netted thread, and fastening into a loop instead of a button hole.
\item The loop of the scabbard of a bayonet or sword. 
\end{enumerate}\end{small}

Les traductions de cette entrée sont divisés en quatre groupes correspondant aux gloses ``\emph{amphibian}'', ``\emph{end of a string instrument’s bow}'', ``\emph{organ in a horse’s foot}'' et ``\emph{part of a railway}''.

De plus parmi les gloses, certaines peuvent contenir des numéros de sens, ajoutés par des utilisateurs de manière ad-hoc (peuvent ou non être présentes, et même si elles le sont, aucun format systématique n'est suivi ou imposé.) Il en suit que la présence d'information de désambiguïsation est très irrégulière et très variable entre les différentes éditions de langues, à la fois en termes de la structure du wiki et de la représentation. 


Dans l'état courant de l'extracteur Wiktionary, nous extrayons toutes les traductions, et quand c'est possible les gloses associées.  Néanmoins, jusqu'à présent nous n'avions pas exploités l'information contenue dans les gloses pour enrichir et désambiguïser les sens source des relations de traduction.


Comme nous l'avons déjà mentionnés, le contenu et le format des gloses est très variable selong les langues, tant qualitativement qie quantitativement.

En effet, comme le montre le tableau \ref{lemon-elts}, certaines langues telles que l'italien ne contiennent aucunement de gloses, pour d'autres, comme l'anglais, il y a des gloses textuelles mais pas de numéros de sens. Pour d'autres encore, telles que l'allemand ne contiennent presque pas de gloses textuelles, mais donnent systématiquement le numéro de sens. Dans les cas spécifique du français, du finnois et du portugais, beaucoup de liens de traductions ont à la fois une glose textuelle rattachée et une numéro de sens.

Dans le but d'évaluer notre méthode, nous avons décidé d'exploiter ces gloses qui contiennent à l fois une description textuelle et un numéro de sens.


\subsubsection{Création d'un étalon-or}
Il arrive souvent que parmi les gloses de traduction disponibles qui contiennent une information textuelle ou un numéro de sens qu'il y ait des faux positifs à la fois du à une extraction approximative à cause de la variabilité des la structure des gloses. Avant d'aller plus loin il est essentiel de filtrer l'information présente afin de ne conserver que les parties pertinentes.

Plus concrètement, deux étapes sont nécessaire pour mener à bien l'extraction des informations nécessaires:
\begin{itemize}
	\item Supprimer les gloses vides ou contenant des informations textuelles non pertinentes qui correspondent souvent à des notes de type \emph{à faire} (par ex. ``\emph{traduction à vérifier}'').
	\item Extraire les numéros de sens des gloses qui en possèdent un, en utilisant des patrons spécifiques à chaque langue (par ex. ``\emph{glose textuelle (1)}'' ou ``\emph{1. glose textuelle}'')
\end{itemize}


Une fois il y avait quantité suffisante de ces gloses (dans telle ou telle édition de langue) nous avons supprimés les numéro s de sens\footnote{Les traductions correspondent parfois à plusieurs sens} des gloses, et les avons utilisées pour produire un étalon-or suivant le format du programme d'évaluation de trec\_eval.

Après cet étape d'extraction, ont peut désormais passer à la description du processus de désambiguïsation à proprement parler. Même si l'extraction était spécifique à chaque langue, la méthode de désambiguïsation a été conçue pour être aussi générique et calculatoirememnt efficiente que possible, sachant que l'on est amenés à effectuer la désambiguïsation périodiquement dès qu'une nouvelle version de DBnary est extraite de Wiktionary.




\section{Attaching Translations to Word Senses}
\subsection{Formalization of translation disambiguation}
Soit \(T\) l'ensemble des relations de traduction, \(L\) l'ensemble des \verb|LexicalEntry| dans une édition de langue donnée de DBnary. Soit \(T_i\in T: Gloss(T_i)\) une fonction qui renvoie la glose d'une relation de traduction quelconque \(T_i\in T\) et soit \(Source(T_i)=L_{T_i}\) une fonction qui renvoie une référence vers la \verb|LexicalEntry| source \(L_{T_i}\) d'une relation de traduction quelconque \(T_i\). soit \(Senses(L_i)=S_{L_i}\) l'ensemble des sens associées à une  \verb|LexicalEntry| \(L_i\). Soit \(S_{L_i}^k\) le \(k\)-ème sens contenus dans \(S_{L_i}\) et soit \(Def(S_{L_i}^k)\) une fonction qui renvoie la définitions textuelle d'un sens  \(S_{L_i}^k\). Enfin, soit \(Sim(A,B)\) une fonction qui renvoie  une mesure de similarité ou un score de relation sémantique  entre \(A\) et \(B\), où \(A,B\) est une paire de définitions textuelles ou de gloses textuelles. 

Ainsi, nous pouvons exprimer le processus de désambiguïsation comme:
\[
\forall T_i \in T, S=Senses(Source(T_i)): 
\]
\[
Source^*(T_i) \leftarrow  \argmax_{S^k\in S} \{Score(Gloss(T_i),Def(S^k))\}
\]

Ceci correspond exactement à une maximisation de la mesure de similarité à postériori et résulte sur un sens désambiguïsé par lien de traduction. Cependant, il arrive dans de nombreux cas qu'un lien de traduction ne correspond à deux sens ou plus à la fois. La solution adoptée par \cite{MeyerGurevych:oup2012} est d'utiliser une valeur seuil \(k\) pour le calcul de la maximisation de la cardinalité de l'intersection des gloses. Dans notre cas, comme nous voulons évaluer et tester pluseurs mesures, sans que l'on puisse nécessairement garantir la normalisation des valeurs de sortie, une \(k\) correspondant à une valeur fixe n'est pas garanti d'être approprié, même si dans la majorité des cas il faut se tenir à la contrainte que les valeurs des mesures de similarités soient normalisées entre  \(0 \mbox{ et  } 1\).

Au lieu de prendre un \(k\) fixe nous avons choisi d'utiliser une fenêtre \(\delta\) autour de sens sélectionne avec le score maximum, ce qui est plus robuste, notamment dans les cas où les valeurs de similarités ne sont pas sur la même échelle. Tout autre sens tombant dans la fenêtre est aussi accepté comme désambiguïsation.  

En comparaison, l'effet d'utiliser une valeur seuil fixe est que si tous les scores sont bas, aucun sens ne sera assigné, on gagne ainsi en précision au prix d'un rappel inférieur. C'est un comportement qui est en temps normal désirable, car détecter les erreur a postériori est difficile, cependant dans le cadre de cette expérience, garder les choix erronés en cas de scores bas nous permet ensuite à l'aide de l'étalon-or de repérer précisément les erreurs potentielles et de mieux les analyser et identifier leur origine exacte. 

Pour intégrer le seuil, nous pouvons modifier la fonction \(argmax\) comme suit:
\[
\forall T_i \in T, S=Senses(Source(T_i)): 
\]
\[
M_S = \max_{S_k\in S}(Score((Gloss(T_i),Def(S^k))),
\]
\[
\argmax_{S_i\in S}^\delta \{Score(Gloss(T_i),Def(S^k))\}=
\]
\[
\{S^k\in S|  M_S > Score((Gloss(T_i),Def(S^k)) > M_S-\delta \}
\]

\subsection{Mesure de similarité}
Afin de désambiguïser les relations de traduction, nous avons besoin de d'utiliser une mesure de similarité sémantique. Sachant que d'une part les seules informations disponibles sont les gloses qui représentent ou résument la définition du sens correspondant et sachant que d'autre part nous avons des définitions textuelles au niveau des sens, il nous faut une mesure de similarité qui ne se basent que sur les chaines de caractères et les mots. 
La mesure de Lesk \cite{citeulike:625530} est un mesure de similarité très simple et standard, qui s'adapte tout particulièrement aux contraintes auxquelles nous sommes confrontés. Cette mesure calcule dans sa formulation classique la cardinalité de l'intersection de deux ensemble de mots (typiquement des définitions de sens de mot).
 Cependant, vienne s'ajouter à cela un certain nombres de limitations importantes :
\begin{itemize}
	\item Si les tailles de gloses ou définitions n'est pas la même, cette mesure favorisera toujours les définitions plus longues. 
	\item La taille et l'adéquation des mots contenus dans les définitions est important, en effet, si il manque un mot clef dans la définition on peut facilement obtenir une similarité erronée (par exemple un score de zéro alors que l'idée est quand même la même, quand par exemple l'une des définition est formulée avec des synonymes du contenu de l'autre).  
	\item La mesure de Lesk classique c'est pas normalisée, c'est à dire que les valeurs peuvent être arbitrairement grandes. L'ajout d'une normalisation n'est pas toujours trivial, et dépend de nombreux facteurs, y compris le domaine d'application.
\end{itemize}
 
 
 Les problèmes de longueurs différentes de définitions et de la normalisation sont en réalité liés. Par exemple un moyen courant de normaliser la mesure est de diviser le score par la longueur de la définition la plus courte ou la plus longue.
 Un autre point intéressant est de remarquer la similarité entre la mesure de Lesk et les coefficients de Dice ou les indices de Jaccard/Tanimoto. En réalité toutes ces mesures sont des cas particulier du modèle plus général de l'indice de Tversky, qui provient des recherches sur la similarité en psychologie cognitive par A. Tversky \cite{tversky77similarity}.

L'indice de Tversky peut être défini de la manière suivante. Soit \(s_1 \in Senses(L_1)\)  et \(s_2 \in Senses(L_2)\), les sens de deux entrées lexicales \(L_1\) et \(L_2\). Soit \(d_i=Def(s_i)\) la définition de \(s_i\), représentée par un ensemble de mots. Alors, la similarité \(Score(s_1, s_2)\) entre les sens s'exprime comme :
\[
Score(s_1,s_2) = 
\frac{|d_1\cap d_2|}{|d_1\cap d_2| + \alpha |d_1-d_2| + \beta |d_2-d_1|}
\]

La mesure peut être généralisée d'avantage en suivant la proposition de \cite{DBLP:conf/otm/PirroE10} en remplaçant la fonction de cardinalité par une fonction quelconque \(F\). Selon les valeurs d'\(\alpha\) et de \(\beta\), l'indice de Tversky prends, comme nous l'avons mentionnés la forme d'autres indices ou coefficients. Pour \((\alpha=\beta=0.5)\) il est équivalent au coefficient de  Dice, et pour \((\alpha=\beta=1)\) à l'indice de Tanimoto. Plus généralement les valeurs de   \(\alpha\) et \(\beta\) expriment combien d'emphase ont attribue aux points communs ou aux différences d'un sens ou de l'autre.

L'indice de Tversky en lui même n'est pas une mesure où une similarité dans le sens mathématique du terme, car en effet il n'est n'y symétrique, ni ne respecte l'inégalité triangulaire, cependant une formulation symétrique à été proposée par 
\cite{Jimenez2010}  pour les cas où c'est une propriété qui est requise. 
% : 
% 
% \[
% S(s_1,s_2) = \frac{|d_1\cap d_2|}{|d_1\cap d_2| + \beta (\alpha a + (1-\alpha)b)}\]
% \[
% a=min(|d_1\cap d_2|,|d_2\cap d_1|)\\
% \]\[
% b=max(|d_1\cap d_2|,|d_2\cap d_1|)\\ 
% \]
 
 Lors de l'utilisation de l'index de Tversky où des ces variantes, une attention toute particulière doit être portée au choix des poids, car selon les applications différents poids vont avoir une très grande influence sur les résultats. 

Dans notre contexte, nous n'avons pas de besoin particulier que la mesure soit symétrique, nous nous cantonnons donc à la forme de base de l'indice. 
 
\subsubsection{Contexte Multilingue et cardinalité partielle}
Quand on travaille sur une seule langue tel que l'Anglais ou le Français, nous avons à notre disposition des outils tels que des lematiseurs, ou encore des racinisateurs, qui peuvent aider à normaliser les formes de mots présentes à une représentation canonique. Ainsi nous pouvons espérer maximiser les cardinalité d'intersection en réduisant la parcimonie habituelles des définitions de sens ou encore plus des gloses. 

De plus pour dans langues agglutinantes telles que le finnois ou l'allemand où encore pour des langues fléchies (par exemple dans la langue Bangalie, les racones communes ne font souvent qu'un caractère, ce qui rends la racination difficile à exploiter).

 For agglutinative languages like German or Finnish, highly inflective language (for example in the Bangla language, common stems are often composed of a single character, which makes stemming difficult to exploit) or languages with no clear segmentation, the preprocessing steps are paramount in order to make overlap based measures viable. If one is working on a single language, even if stemmers and lemmatizers do not exist, it is not impossible to build such a tool.

However, in the context of this work we are currently dealing with 10 languages (and potentially in the future with all the languages present in Wiktionary) and thus, in order to propose a truly general method, we cannot expect the prerequisite presence of such tools. 

How then, can we manage to compute overlaps effectively? When computing Lesk, if two words overlap, the score is increased by 1 and of two words do not overlap, the overlap does not change. What if we had a way to count meaningful partial overlaps between words and instead of adding 1, we may add whatever values between 0 and 1 represents the amount of overlap.

The simplest approach to the problem is to use some form of partial string matching metric to compute partial overlaps, a seemingly trivial and classical approach that can, however, greatly improve the result as we shall endeavour to elicit and as has already been extensively shown by \cite{Jimenez2012}. 

As mentioned in the Related Work section, there are many approximate string matching measures as reviewed by \cite{Cohen2003}. We integrate these measures in the Teversky index by setting the \(F\) function that replaces the set cardinality function appropriately (a simplified version of soft cardinality):

\[
	A \mbox{ , a set} : F(A) = (\sum_{A_i,A_j \in A}sim(A_i, A_j))^{-1}
\]

In our case, \(sim\) will be an string distance measure.

\subsubsection{Longest Common Substring Constraints}
With this similarity measure, we are mainly interested in capturing word that have common stems, without the need for a stemmer, as such we do not for example want to consider the overlap of prefixes or suffices, as they do not carry the main semantic information of the word. If two words only match by a common suffix that happens to be used very often in that particular language, we will have a non-zero overlap, but we would have captures no sematic information whatsoever. Thus, in this word we put a threshold of a longest common subsequence of three characters.

\section{Experiments}
\label{sec:expe}
As was mentioned previously, we have been able to extract gold standards from the sense numbered textual glosses of translations in certain language editions of Wiktionary. Then we stripped all sense number information from the glosses, so we could disambiguate those same translation and then evaluate the results on the previously generated gold standard.

We will first describe how we generated the Gold standard and the tools and measures used for the evaluation. We will then proceed onto the empirical selection of the best parameters for our Tversky index as well as the most appropriate string distance measure to use for the fuzzy or soft cardinality. Then, we will compare the results of the optimal Tversky index with other Level 2 similarity measures.
 
\subsection{Evaluation}
Let us first describe the Gold Standard generation process, then proceed on to describing how we represented the Gold Standard in Trec\_eval format, a scorer program from the query answering Trec\_Eval campaign. Finally we will described the evaluation measures we will use. 
\subsection{Gold Standard}
Only certain languages meet the requirements for the generation of a sufficiently large Gold Standard. To be more specific, we could only chose among languages where:
\begin{enumerate}
	\item There are textual glosses (for the overlap measures)
	\item There are numbers in said glosses indicating the right sense number
	\item The above are available in a sufficient quantity (at least a few thousand)
\end{enumerate}

Four languages could potentially meet the criteria (see the last column of Table \ref{lemon-elts}): French, Portuguese, Finnish and Japanese, however we could only manage the time to extract gold standards for French, Portuguese and Finnish.

\subsubsection{Trec\_eval, scoring as a query answering task}

A query answering task is more generally a multiple-labelling problem, which is exactly equivalent to what we are producing when we use the threshold \(\delta\). Here, we can consider that each translation number is the query identifier and that each sense URI is a document identifier. We answer the "translation" queries by providing one or more senses and an associated weight.

Thus, we can generate the gold standard and the results in the Trec\_eval format, the very complete scorer for and information retrieval evaluation campaign of the same name.

\subsubsection{Measures}
We will use the standard set matching metrics used in Information Retrival and Word Sense Disambiguation, namely Recall, Precision and F\_{1} measure. Where, \(P=\frac{|\{Relevant\}\cap\{Disambiguated\}|}{|\{Disambiguated\}|}\), \(R=\frac{|\{Relevant\}\cap\{Disambiguated\}|}{|\{Relevant\}|}\), and \(F_1 = \frac{2\cdot P \cdot R}{P + R} \), the harmonic mean of \(R\) and \(P\). However, for the first step consisting in the estimation of the optimal parameters, we will only provide the \(F_1 score\), as we are interested in maximising both recall and precision in an equal fashion.
\subsection{Similarity Measure Tuning}
There are several parameters to set in our Tversky index, however the first step is to find the most suitable string distance measure.

\subsubsection{Optimal String Distance Metric}
 The \(\delta\) parameter influences performance independently of the similarity measure, so we can first operate with \(\delta=0\), which restricts us to a single disambiguation per translation. Furthermore, the weights of the Tvsersky index are applied downstream from the string edit distance, and thus does not influence the relative performance of the different string distance metrics combined to our Tversky index. In simple terms, the ratio of the tverski indices computed on different measures is constant, independently of \(\alpha\) and \(\beta\).  Thus for this first experiment, we will set \(\alpha=\beta=0.5\), in other words the index becomes the dice coefficient.

As for the selection of the string similarity measures to compare, we take the best performing measures from \cite{Cohen2003}, namely Jaro-Winkler, Monge-Elkan, Scaled Levenshtein Distance, to which we also add the longest common substring for reference. As a baseline measure, we will use the Tversky index with a standard overlap cardinality.

We give the following short notations for the measures: Tversky Index -- Ts; Jaro-Winkler -- JW; Monge-Elkan -- ME; Scaled Levenshtein -- Ls; Longest Common Substring -- Lcss; F -- Fuzzy. For example standard Tversky index with classical cardinality shall be referred to as "Ti", while the fuzzy cardinality version with a Monge-Elkan string distance shall be referred to as "FTiME". 

Table \ref{tab:expe1} presents the results for each string similarity measure and each of the languages (Fr, Fi, Pt).

As we can see, for all language, the best string similarity measure is clearly the scaled Levenstein measure as it systematically exhibits a score higher from \(+1\%\) to \(+1.96\%\).

\begin{table}
{\centering \footnotesize
\begin{tabular}{|c|c|c|c|}
\hline &French&Portuguese&Finnish\\
\hline &F1&F1&F1\\
\hline FTiJW&0.7853&0.8079&0.9479\\
\hline FTiLcss&0.7778&0.7697&0.9495\\
\hline FTiLs&\textbf{0.7861}&\textbf{0.8176}&\textbf{0.9536}\\
\hline FTiME&0.7684&0.7683&0.9495\\
\hline Ti&0.7088&0.7171&0.8806\\
\hline 
\end{tabular} 
\caption{Results comparing the performance in terms of F\_1 score for French, Finnish and Portuguese (highest scores in bold).}
\label{tab:expe1}
}
\end{table}

\subsubsection{Optimal \(\alpha\), \(\beta\) selection}
Now that we have found the optimal string distance measure, we can look for the optimal ratio of \(\alpha\) and \(\beta\). Here, we will keep both values complementary, that is \(\alpha=1-\beta\) so as to obtain more balances score that remain in the 0 to 1 range. 

Given that translation glosses are short (often a single word), it is likely that the optimum is around \(\alpha=1-\beta=0.1\), given that what interests us is  that the word in the translation gloss matches with less importance for the remaining words in the sense definition that do not match.

We chose, here, to evaluate the values of \(\alpha\) and \(\beta\) in steps of \(0.1\). Figure \ref{fig.1} graphically shows the \(F\_1\) score for each pair of values of \(alpha\) and \(beta\) for all three languages. We can indeed confirm our hypothesis as the optimal value in all three cases is indeed \(\alpha=1-\beta=0.1\) with a difference between \(+0.15\%\) to \(+0.43\%\) with the second best scores.
	
\begin{figure}\centering
\includegraphics[width=0.45\columnwidth]{alphabetafig}
\caption{F1 score for Finnish, French and Portuguese depending on the value of \(\alpha\) and \(\beta\).}
\label{fig.1}
\end{figure}

\subsubsection{Optimal \(\delta\) selection}
Now that we have fixed the best values of \(\alpha\) and \(\beta\) we can search for the best value for \(\delta\). We make delta vary in steps of \(0.05\) between \(0\) and \(0.3\). The choice of the upper bound is based on the hypothesis that the optimal value is somewhere closer to 0, as a too large threshold essentially means that most of the senses for each translation might be considered as corresponding to the translation at hand and thus it should drastically reduce performance. 

The \(\delta\) heuristic affects the results of the disambiguation whether the measure is  Tversky index or another Level 2 Textual similarity. Thus, in this experiment, we will also include Level 2 version of the three string distance measures that we used in the first experiment.

Figure \ref{fig.2} graphically presents the \(F_1\) scores for each value of \(delta\) and each language. The first apparent trend is that Level 2 measures are systemically performing much worse (by up to 30\%) than our own similarity measure. Depending on the language different values of \(delta\) are optimal, even though it is difficult to see a great difference. For French \(\delta=0.10\), for Finnish \(\delta=0.15\)
 and for Portuguese \(\delta=0.10\). 
In all three previous experiments, it became apparent, that the same string similarity measure, the same values for alpha and beta as well as the same value for delta were optimal, which leads us to believe that their optimality will be conserved across all languages. However, especially for the string similarity measure, it is reasonable to believe that for languages such a Chinese or Japanese that lack segmentation, the optimal choice for the string distance measure may be entirely different.
\subsection{Final Disambigation Results}

Now that we have found all the optimal parameters, we can actually present the final results combining all the optimal parameters. They are in fact the very results that were optimal in the previous experiment. We shall now take these results and place them in a separate table (Table \ref{tab:final}) so as to make the comparison analysis easier. We will use the chance of random selection as well as the most frequent sense selection as baseline for this comparison.

The first thing one can notice is that there is a stark difference between the scores of Finnish, and the rest. Indeed, first of all the random baseline and most frequent sense baselines are an indication that the French and Portuguese DBNaries are highly polysemous, while Finnish contains a very large amount of monosemous entries, which artificially inflates the value of the score. 

Another point of interest is that the random baseline is higher (up to 6.6\%) than the most frequent sense baseline, which indicates that the first sense if often not the right sense to select to match the translation. 

We can see that for all three languages we achieve a good performance compared to what is presented in the literature, most notably in the fact that most of the errors, can easily be identified as such just by looking at whether or not it produced any overlap.

\begin{table}
{\centering \footnotesize
\begin{tabular}{|c|c|c|c|c|c|}
\hline &P&R&F1&MFS F1&Random\\
\hline Portuguese&0.8572&0.8814&0.8651&0.2397&0.3103\\
\hline Finnish&0.9642&0.9777&0.9687&0.7218&0.7962\\
\hline French&0.8267&0.8313&0.8263&0.3542&0.3767\\
\hline 
\end{tabular}
\caption{Final results with optimal measure and parameter values. Precision, Recall, F1 measure for all three languages compared against the MFS and Random Baselines.}
\label{tab:final}
}
\end{table} 

\begin{figure}
\centering
\includegraphics[width=.35\columnwidth]{french}
\includegraphics[width=.35\columnwidth]{portuguese}
\includegraphics[width=.35\columnwidth]{finnish}
\caption{Graphical representation of the F1 score against delta for our measure and other Level 2 Measures.}
\label{fig.2}
\end{figure}

\subsection{Error analysis}

We did not here perform a full fledged and systematic error analysis, but rather an informal manual sampling so as to have an idea of what the error can be and if there are ways to correct them by adapting the measures or the methodology.
We looked at some of the error made by the disambiguation process and manually checked them so as to categorize them. We found three main categories:
\begin{enumerate}
\item No overlap between the gloss and sense definitions (Random choice by our algorithm), this happens when the translation gloss is a paraphrase of the sense definition or simply a metaphor for it.
\item The overlap is with the domain category label or the example glosses, which we do not currently extract. This is a particular case of the first type of error.
\item New senses have been introduced in Wiktionary and shifted sense numbers, which were not subsequently updated in the resource. Such errors cannot be detected during the extraction process.
\end{enumerate}

We can in fact easily find all the errors due to the lack of overlap and correct the errors of type 2 by enriching the extraction process of DBnary. Thus we can single out errors that are due to inconsistencies in the resource and thus potentially use the disambiguation results to indicate to users where errors are located an need to be updated.

\section{Conclusion}

With our method, we were able to determine and optimal similarity measure for disambiguating translation in DBnary. Similar results across the three evaluation languages suggests that it is a general optimality that can be applied to all the languages currently present in DBnary, although for Asian Languages that have no segmentation, it is likely not the case.

Then, we compared the results and concluded that our method is viable for the task of disambiguating glossed translation relations, especially considering the low random baselines and first sense baselines compared to the top score of our disambiguation method.

For translation relations without glosses, the disambiguation process is more complex and is part of the Future Work that we plan on carrying out.


%%================================================================
\section*{Remerciements (pas de numéro)}

Paragraphe facultatif

%%================================================================
%% Note : si l'on préfère éviter de factoriser les crossrefs :
%% bibtex -min-crossrefs 99 taln-exemple
%%================================================================
\bibliographystyle{taln2002}
\bibliography{taln-dbnary-wsd}

%%================================================================
\end{document}
